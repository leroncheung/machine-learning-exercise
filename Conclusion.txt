plot函数的参数意义：
  Format arguments:
     linestyle
          '-'  Use solid lines (default).
          '--' Use dashed lines.
          ':'  Use dotted lines.
          '-.' Use dash-dotted lines.
     marker
          '+'  crosshair
          'o'  circle
          '*'  star
          '.'  point
          'x'  cross
          's'  square
          'd'  diamond
          '^'  upward-facing triangle
          'v'  downward-facing triangle
          '>'  right-facing triangle
          '<'  left-facing triangle
          'p'  pentagram
          'h'  hexagram
     color
          'k'  blacK
          'r'  Red
          'g'  Green
          'b'  Blue
          'y'  Yellow
          'm'  Magenta
          'c'  Cyan
          'w'  White

figure
plot
subplot
theta0_vals = linspace(-10, 10, 100);
theta1_vals = linspace(-1, 4, 100);
surf(theta0_vals, theta1_vals, Jvals);
contour(theta0_vals, theta1_vals, Jvals, logspace(-2, 3, 20));
hold on;
hold off;
=======================================================================================
% Linear regression

data = load('data1.txt');
data = csvread('data1.txt');	% Use csvread function load data
X = data(:, 1);
y = data(:, 2);
m = length(y);

X = [ones(m, 1), X];
theta = zeros(size(X, 2) + 1, 1); % (n + 1) * 1 vector, Initial theta values

% Cost compute
J = computeCost(X, y, theta);
%%%
function J = computeCost(X, y, theta)
J = 0;
J = 1 / (2 * m) * sum((X * theta - y) .^ 2);
end

% Gradient descent
iterations = 400;
alpha = 0.1;
theta = gradientDescent(X, y, theta, iterations, alpha);
%%%
function theta = gradientDescent(X, y, theta, iterations, alpha)
J_history = zeros(iterations 1);
for iter = 1: iterations,
	delta = 1 / m * sum(X' * (X * theta - y));
	theta = theta - alpha * delta;
	J(iter) = computeCost(X, y, theta);
end
end

=======================================================================
[X, mu, sigma] = featureNormalize(X);
%%%
function [X, mu, sigma] = featureNormalize(X)
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));
mu = mean(X);
sigma = std(X;
X = (X - mu) ./ sigma;
end


% Normal equation
theta = normalEqn(X, y);	% Has already added intercept term into X
%%%
function theta = function normalEqn(X, y)
theta = zeros(size(X, 2), 1);
theta = pinv(X' * X) * X' * y;
end



==================================
% Logistic regression

pos = find(y == 1);
neg = find(y == 0);
plot(X(pos, 1), X(pos, 2), r+, 'MarkerSize', 7);
plot(X(neg, 1), X(neg, 2), bo, 'MarkerSize', 7);

[m, n] = size(X);
theta = zeros(n + 1, 1);
X = [ones(m, 1), X];

% Sigmoid Function
%%%
function hypothesis = sigmoid(z)
hypothesis = zeros(size(z));	% z == X * theta
hypothesis = 1 ./ (1 + exp(-z));
end

% costFunction and gradient
J = costFunction(X, y, theta);
%%%
function [J, grad] = costFunction(X, y, theta)
J = 0;
grad = zeros(size(X, 2) + 1, 1);
hypothesis = sigmoid(X * theta);
J = -1 / m * (y' * log(hypothesis) + (1 - y)' * log(1 - hypothesis));
grad = 1 / m * X * (y - hypothesis);
end

==================================
options = optimset('GradObj', 'on', 'MaxIter', 400);
[theta, grad] = fminunc(@(t)(costFunction(t, X, y)), theta, options);

==============================================================================
% Regularlized Logistic regression

X = mapfeature(X(:, 1), X(:, 2));

theta = zeros(size(X, 2), 1);
lamda = 1;

[cost, grad] = costFunctionReg(theta, X, y, lambda);
%%%
function [cost, grad] = costFunctionReg(theta, X, y, lamda);
cost = 0;
grad = zeros(size(X, 2), 1);

hypothesis = sigmoid(X * theta);
theta(1, 1) = 0;	% 【VERY IMPORTANT!!!!】 
cost = -1 / m * (y' * log(hypothesis) + (1 - y)' * log(1 - hypothesis)) + lamda / (2 * m) * sum(theta .^ 2);
grad = 1 / m * X * (y - hypothesis) + lambda / m * theta;
end


